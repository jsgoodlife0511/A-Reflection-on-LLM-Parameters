{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOXtsV8z6aiH"
      },
      "outputs": [],
      "source": [
        "# Library install\n",
        "!pip install transformers==4.48.0 peft==0.14.0 trl==0.13.0 bitsandbytes==0.45.3 accelerate==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKm5EZij6t2N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Load Transformer model\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B\", # Loading a base model without quantization before applygin QLoRA\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map = {\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "id": "d4rNgZM0MQos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google drive mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8hzdbZuo59Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_adapter_path = '/content/drive/MyDrive/fine_tune_output/checkpoint-50'"
      ],
      "metadata": {
        "id": "Pq_tvH4w5_6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Llama3 base model + adapter (==the merged model in part 1)\n",
        "from peft import PeftConfig, PeftModel\n",
        "base_and_adapter_model = PeftModel.from_pretrained(model, fine_tuned_adapter_path) # Attaching the merged model in part 1 as an adapter"
      ],
      "metadata": {
        "id": "SNhIiAaBMXIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_and_adapter_model = base_and_adapter_model.merge_and_unload() # Merging the base model with the QLoRA adapter and generated a new adapter-free base model"
      ],
      "metadata": {
        "id": "kzDsMfft5yAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_adapter_path)"
      ],
      "metadata": {
        "id": "ZOs8_Z7XI--z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"jsgoodlife0511/llama3.1-tuned-and-merged\""
      ],
      "metadata": {
        "id": "B0h_8j-5PhqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_and_adapter_model.push_to_hub(model_path)"
      ],
      "metadata": {
        "id": "gxDhNkr5-F5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.push_to_hub(model_path)"
      ],
      "metadata": {
        "id": "66oVnwICJLBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}