{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token"
      ],
      "metadata": {
        "id": "dVXgexY0Mp7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Djpt9VTnsHAH"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.48.0 peft==0.14.0 trl==0.12.0 bitsandbytes==0.45.3 accelerate==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4rNgZM0MQos"
      },
      "outputs": [],
      "source": [
        "# Quantization config\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quantization_config=BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNhIiAaBMXIU"
      },
      "outputs": [],
      "source": [
        "# PEFT(Parameter Efficient Fine Tuning) - Setting LoRA\n",
        "from peft import LoraConfig\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16, # scaling factor applied to BA when updating weights\n",
        "    lora_dropout=0,\n",
        "    r=16, # rank of the low-ranked matrices\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRAxt0yjM9N1"
      },
      "outputs": [],
      "source": [
        "# Load a base Llama model\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    quantization_config = quantization_config, # A base model quantized to 4 bit - cannot train this base model\n",
        "    device_map = {\"\": 0}                       # Instead, we can fine-tune this with QLoRA.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALbsxvDXAV41"
      },
      "outputs": [],
      "source": [
        "# Set Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\") # Each model has its own tokenizer\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<|reserved_special_token_250|>\"}) # Ensure the special token has consistent length\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqCHDCtJAicE"
      },
      "outputs": [],
      "source": [
        "# Instruction Formatting (Prompt/Response Format) - very important !!\n",
        "# When you train an LLM, the instruction format you use in the training data should be the same\n",
        "# as the format you use when asking the model questions later.\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def convert_to_alpaca_format(instruction, response):\n",
        "    alpaca_format_str = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\\n",
        "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\\\n",
        "    \"\"\"\n",
        "\n",
        "    return alpaca_format_str\n",
        "''' (Sample instruction format below)\n",
        "Below is an insruction that describe a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "What is LLM\n",
        "\n",
        "### Response:\n",
        "LLM is an AI model. <--(Before output is generated, this line is empty)\n",
        "'''\n",
        "\n",
        "def prompt_formatting_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = [] # Adding EOS_TOKEN is very important for a model to learn when to stop generating text\n",
        "    for instruction, output in zip(instructions, outputs): # Create a single tuple per iteration by zip\n",
        "        alpaca_formatted_str = convert_to_alpaca_format(instruction, output) + EOS_TOKEN # Don't forget adding end of text token!!\n",
        "        texts.append(alpaca_formatted_str)\n",
        "    return { \"text\" : texts, }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3LpzRjSEOLl"
      },
      "outputs": [],
      "source": [
        "# Dataset Load\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
        "\n",
        "dataset = dataset.shuffle(seed=42) # Ensure fine-tuning result is reserved\n",
        "no_input_dataset = dataset.filter(lambda example: example['input'] == '') # A bit different from typical filter() in Python 3. Data parameter is skipped\n",
        "                                                                          # 'input' column is not needed. We use only 'instruction' and 'output' column\n",
        "mapped_dataset = no_input_dataset.map(prompt_formatting_func, batched=True) # Get a map object from 'prompt_formatting_func'\n",
        "split_dataset = mapped_dataset.train_test_split(test_size=0.01, seed=42)\n",
        "\n",
        "train_dataset = split_dataset['train']\n",
        "test_dataset = split_dataset['test']\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2st7_0JV7CdU"
      },
      "outputs": [],
      "source": [
        "# Data Collator\n",
        "# 1) Apply tokenizer for padding token\n",
        "# 2) ** set response template for a model to learn how to answer (how to generate text) **\n",
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "data_collator_param = {}\n",
        "response_template = \"### Response:\\n\" # 2) The part that follows this 'Response' is where the weights should be updated through the loss function\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer, mlm=False)\n",
        "data_collator_param[\"data_collator\"] = collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJNtEZ1gAVWW"
      },
      "outputs": [],
      "source": [
        "# local output dir\n",
        "local_output_dir = \"/content/fine_tune_output\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oR8G4NgAYh2"
      },
      "outputs": [],
      "source": [
        "!mkdir {local_output_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l-fadt2_FKk"
      },
      "outputs": [],
      "source": [
        "# tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '{local_output_dir}/runs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTChjxB3_H7G"
      },
      "outputs": [],
      "source": [
        "# Training setup\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "  output_dir=local_output_dir,\n",
        "  report_to = \"tensorboard\",\n",
        "  per_device_train_batch_size = 2,\n",
        "  per_device_eval_batch_size = 2,\n",
        "  gradient_accumulation_steps = 8,\n",
        "  warmup_steps = 50,\n",
        "  max_steps = 100, # Total number of training samples = 2 (samples per batch) * 50 (max steps)\n",
        "  eval_steps=10,   # The weights will be updated 100 times (due to max_steps == 100)\n",
        "  save_steps=50,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  save_strategy=\"steps\",\n",
        "  learning_rate = 1e-4,\n",
        "  logging_steps = 1,\n",
        "  optim = \"adamw_8bit\",\n",
        "  weight_decay = 0.01,\n",
        "  lr_scheduler_type = \"constant_with_warmup\",\n",
        "  seed = 42,\n",
        "  gradient_checkpointing = True,\n",
        "  gradient_checkpointing_kwargs={'use_reentrant':True}\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer( # Trainer optimized for supervised fine-tuning\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "     peft_config=peft_config, # Depending on trainer, either 1) Both base model and peft model should be provided // OR 2) Only peft model is needed\n",
        "    dataset_text_field = \"text\", # prompt text for training\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = training_arguments,\n",
        "    **data_collator_param\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKNP2hiV_KoD"
      },
      "outputs": [],
      "source": [
        "train_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJA3GiBU_MNr"
      },
      "outputs": [],
      "source": [
        "# Copy to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcEXP6-ALYp-"
      },
      "source": [
        "!ls /content/fine_tune_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iCY-vKVAkCX"
      },
      "outputs": [],
      "source": [
        "!cp -r {local_output_dir} /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP4xH0TdLcpB"
      },
      "outputs": [],
      "source": [
        "!ls /content/fine_tune_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Jswoleur4ocS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}