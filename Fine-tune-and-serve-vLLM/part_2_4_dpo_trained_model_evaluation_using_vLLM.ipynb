{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOXtsV8z6aiH"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.55.2 peft==0.17.0 trl==0.21.0 bitsandbytes==0.47.0 accelerate==1.10.0 vllm==0.10.1 gradio==5.43.0 pydantic==2.11.7\n",
        "!pip install ipython>=8.0 jedi>=0.19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKm5EZij6t2N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive Import\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qkO0Kyw2tR-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/fine_tune_output"
      ],
      "metadata": {
        "id": "zoo1QIdStTem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/dpo_output_1"
      ],
      "metadata": {
        "id": "d4rNgZM0MQos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/dpo_output_2"
      ],
      "metadata": {
        "id": "lRAxt0yjM9N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from vllm import LLM, SamplingParams\n",
        "import gradio as gr\n",
        "\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "vllm_model = LLM(\n",
        "    model = \"Evion/llama3-alpaca-tuned-and-merged\",\n",
        "    tokenizer = \"Evion/llama3-alpaca-tuned-and-merged\",\n",
        "    gpu_memory_utilization=0.85,\n",
        "    enable_lora = True\n",
        ")\n",
        "\n",
        "sampling_params = SamplingParams(temperature=0.05, top_p=0.95, max_tokens=256)"
      ],
      "metadata": {
        "id": "DjDX3wmPr-8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALbsxvDXAV41"
      },
      "outputs": [],
      "source": [
        "from vllm.lora.request import LoRARequest\n",
        "\n",
        "dpo_output_1_base_path = \"/content/drive/MyDrive/dpo_output_1/\"\n",
        "dpo_output_2_base_path = \"/content/drive/MyDrive/dpo_output_2/\"\n",
        "\n",
        "lora_configs = {\n",
        "    \"dpo_output_1_cpk_10\": (1, dpo_output_1_base_path + \"checkpoint-10\"),\n",
        "    \"dpo_output_1_cpk_20\": (2, dpo_output_1_base_path + \"checkpoint-20\"),\n",
        "    \"dpo_output_1_cpk_30\": (3, dpo_output_1_base_path + \"checkpoint-30\"),\n",
        "    \"dpo_output_1_cpk_40\": (4, dpo_output_1_base_path + \"checkpoint-40\"),\n",
        "    \"dpo_output_2_cpk_10\": (5, dpo_output_2_base_path + \"checkpoint-10\"),\n",
        "    \"dpo_output_2_cpk_20\": (6, dpo_output_2_base_path + \"checkpoint-20\"),\n",
        "    \"dpo_output_2_cpk_30\": (7, dpo_output_2_base_path + \"checkpoint-30\"),\n",
        "    \"dpo_output_2_cpk_40\": (8, dpo_output_2_base_path + \"checkpoint-40\")\n",
        "}\n",
        "\n",
        "def generate_text(raw_input, temperature = 0.05, top_p = 0.95, max_tokens=256, lora_mode = \"default\"):\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    {}\n",
        "\n",
        "    ### Response:\n",
        "    {}\"\"\"\n",
        "    prompt = alpaca_prompt.format(raw_input,\"\")\n",
        "    lora_config = {}\n",
        "\n",
        "\n",
        "\n",
        "    if lora_mode != \"default\":\n",
        "        lora_config[\"lora_request\"] = LoRARequest(lora_mode, lora_configs[lora_mode][0], lora_configs[lora_mode][1])\n",
        "\n",
        "    outputs = vllm_model.generate(\n",
        "        [prompt],\n",
        "        sampling_params,\n",
        "        **lora_config,\n",
        "    )\n",
        "    return outputs[0].outputs[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3LpzRjSEOLl"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"List three ways to reduce plastic waste in daily life.\",\n",
        "    \"Write a haiku about artificial intelligence\",\n",
        "    \"Translate '안녕하세요, 오늘 날씨가 좋네요' into English.\",\n",
        "    \"Explain what the term LLM means in the field of AI.\",\n",
        "    \"What is a famous tall tower in Paris?\",\n",
        "    \"What is Fine-Tuning?\",\n",
        "    \"Find the least common multiple (LCM) of 15 and 25.\",\n",
        "    \"What were the main causes of World War II?\",\n",
        "    \"Briefly explain the process of photosynthesis.\",\n",
        "    \"Analyze the personality of the protagonist in Shakespeare's 'Hamlet'.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, question in enumerate(questions):\n",
        "  print(f\"Question_{idx}: {question}\")\n",
        "  print(\"Default model response -----\")\n",
        "  print(generate_text(raw_input = question, lora_mode = \"default\"))\n",
        "  for mode in lora_configs.keys():\n",
        "    print(f\"{mode} model response ------\")\n",
        "    print(generate_text(raw_input = question, lora_mode = mode))\n",
        "    print()"
      ],
      "metadata": {
        "id": "7iCY-vKVAkCX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}