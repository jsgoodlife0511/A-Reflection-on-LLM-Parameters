{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token"
      ],
      "metadata": {
        "id": "DhwqkklKCaRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOXtsV8z6aiH"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.48.0 peft==0.14.0 trl==0.13.0 bitsandbytes==0.45.3 accelerate==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qkO0Kyw2tR-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tuned_model_path = \"/content/drive/MyDrive/fine_tune_output/checkpoint-50\""
      ],
      "metadata": {
        "id": "zoo1QIdStTem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization figuration to 4 bits\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quantization_config=BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")"
      ],
      "metadata": {
        "id": "d4rNgZM0MQos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a basic Llama 3 model\n",
        "from transformers import AutoModelForCausalLM\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    quantization_config = quantization_config,\n",
        "    device_map = {\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "id": "lRAxt0yjM9N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load my fine-tuned model\n",
        "fine_tuned_model_cp_50 = AutoModelForCausalLM.from_pretrained(\n",
        "    fine_tuned_model_path,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map = {\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "id": "DjDX3wmPr-8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALbsxvDXAV41"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqCHDCtJAicE"
      },
      "outputs": [],
      "source": [
        "# Prompt/Response Format\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def convert_to_alpaca_format(instruction, response):\n",
        "    alpaca_format_str = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\\n",
        "    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\\\n",
        "    \"\"\"\n",
        "\n",
        "    return alpaca_format_str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(instruction_str, model):\n",
        "    inputs = tokenizer(\n",
        "    [\n",
        "        convert_to_alpaca_format(instruction_str,\"\",) # Response field is empty because it is an inference stage!\n",
        "    ], return_tensors = \"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True, temperature = 0.05, top_p = 0.95) # Basic inference function in HF\n",
        "    return(tokenizer.batch_decode(outputs)[0]) # Decode token IDs into words"
      ],
      "metadata": {
        "id": "dxOHbeMjpF4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3LpzRjSEOLl"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"List three ways to reduce plastic waste in daily life.\",\n",
        "    \"Write a haiku about artificial intelligence.\",\n",
        "    \"Translate '안녕하세요, 오늘 날씨가 좋네요' into English.\",\n",
        "    \"How can I improve my public speaking skills?\",\n",
        "    \"Explain what the term LLM means in the field of AI.\",\n",
        "    \"What is a famous tall tower in Paris?\",\n",
        "    \"What is Fine-Tuning?\",\n",
        "    \"Find the least common multiple of 15 and 25.\",\n",
        "    \"What were the main causes of World War II?\",\n",
        "    \"Briefly explain the process of photosynthesis.\",\n",
        "    \"Analyze the personality of the protagonist in Shakespeare's 'Hamlet'.\",\n",
        "    \"Explain the global impact of climate change.\",\n",
        "    \"What is the difference between a list and a tuple in Python?\",\n",
        "    \"Can artificial intelligence have consciousness? Why or why not?\",\n",
        "    \"Write a short story about space travel.\",\n",
        "    \"Explain the effects of inflation on the economy.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answers_dict = {\n",
        "  \"base_model_answers\": [],\n",
        "  \"fine_tuned_model_answers\": []\n",
        "}\n",
        "for idx, question in enumerate(questions):\n",
        "    print(f\"Processing EXAMPLE {idx}==============\")\n",
        "    base_model_output = test_model(question, base_model)\n",
        "    answers_dict['base_model_answers'].append(base_model_output)\n",
        "    fine_tuned_model_output = test_model(question, fine_tuned_model_cp_50)\n",
        "    answers_dict['fine_tuned_model_answers'].append(fine_tuned_model_output)"
      ],
      "metadata": {
        "id": "7iCY-vKVAkCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_format(text, width=120):\n",
        "    return '\\n'.join(line[i:i+width] for line in text.split('\\n') for i in range(0, len(line), width))\n",
        "\n",
        "\n",
        "for idx, question in enumerate(questions):\n",
        "    print(f\"EXAMPLE {idx}==============\")\n",
        "    print(f\"Question: {question}\")\n",
        "\n",
        "    print(\"<<Base Model response>>\")\n",
        "    base_model_answer = answers_dict[\"base_model_answers\"][idx].split(\"### Response:\")[1]\n",
        "    print(simple_format(base_model_answer))\n",
        "    print(\"---\")\n",
        "    print(\"<<Fine Tuning Model response>>\")\n",
        "    fine_tuned_model_answer = answers_dict[\"fine_tuned_model_answers\"][idx].split(\"### Response:\")[1]\n",
        "    print(simple_format(fine_tuned_model_answer))\n",
        "    print()"
      ],
      "metadata": {
        "id": "jN9rOYCk2XhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}