{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.48.0 peft==0.14.0 trl==0.13.0 bitsandbytes==0.45.3 accelerate==1.2.1"
      ],
      "metadata": {
        "id": "5NewEq5kc7dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ['HF_TOKEN'] = hf_token"
      ],
      "metadata": {
        "id": "I7kHsWL3dvDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "quantization_config=BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")"
      ],
      "metadata": {
        "id": "d4rNgZM0MQos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a merged(tuned) model and quantizing it to 4-bits\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"jsgoodlife0511/llama3.1-tuned-and-merged \",\n",
        "    quantization_config = quantization_config,\n",
        "    device_map = {\"\": 0}\n",
        ")"
      ],
      "metadata": {
        "id": "8hzdbZuo59Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"jsgoodlife0511/llama3.1-tuned-and-merged\")"
      ],
      "metadata": {
        "id": "ZOs8_Z7XI--z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lora\n",
        "\n",
        "from peft import LoraConfig\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"v_proj\",\"k_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "gxDhNkr5-F5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model\n",
        "model = get_peft_model(model, peft_config) # Directly attaching a QLoRA to base model"
      ],
      "metadata": {
        "id": "66oVnwICJLBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt/Response Formatting\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "# Prompt fommating should be the same with the one used in part 1 (when Alpaca dataset was used)\n",
        "# The prompt format represents the \"context structure\" that the model has been trained on, so it needs to remain consistent during DPO training as well.\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "def formatting_prompts_func_for_dpo(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    chosens = examples[\"chosen\"]\n",
        "    rejects = examples[\"rejected\"]\n",
        "\n",
        "    prompt_lst, chosen_lst, rejected_lst = [], [], []\n",
        "    for prompt, chosen, rejected in zip(questions, chosens, rejects):\n",
        "        prompt = alpaca_prompt.format(prompt, \"\")\n",
        "        prompt_lst.append(prompt)\n",
        "        chosen_lst.append(chosen + EOS_TOKEN)\n",
        "        rejected_lst.append(rejected + EOS_TOKEN)\n",
        "\n",
        "    return {\"prompt\" : prompt_lst, \"chosen\": chosen_lst, \"rejected\": rejected_lst,}"
      ],
      "metadata": {
        "id": "OXeUAhSgQ9kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Orca Dataset Load\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"Intel/  s\", split = \"train\")\n",
        "\n",
        "# Remove \"question ...\" dataset. We will use instruction-only data.\n",
        "# Set a length limitation to generate a shorter text than in DPO_training_1.\n",
        "filtered_dataset = dataset.filter(\n",
        "    lambda example:\n",
        "        \"question:\" not in example['question'].lower()\n",
        "        and 'q:' not in example['question'].lower()\n",
        "        and (len(example['rejected']) - len(example['chosen'])) >= 200\n",
        "        and len(example['chosen']) <= 300\n",
        "    )\n",
        "mapped_dataset = filtered_dataset.map(formatting_prompts_func_for_dpo, batched=True, remove_columns=filtered_dataset.column_names)\n",
        "split_dataset = mapped_dataset.train_test_split(test_size=0.05, seed=42)\n",
        "\n",
        "train_dataset = split_dataset['train']\n",
        "test_dataset = split_dataset['test']\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "x66x75VzQr86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_output_dir = \"/content/dpo_output_2\"\n",
        "!mkdir {local_output_dir}"
      ],
      "metadata": {
        "id": "9uIX0ywKQx8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '{local_output_dir}/runs'"
      ],
      "metadata": {
        "id": "b1BYhYatQy2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "from trl.trainer.dpo_config import DPOConfig\n",
        "\n",
        "training_args = DPOConfig(\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type = \"constant_with_warmup\",\n",
        "    max_steps=300,\n",
        "    eval_steps=10,\n",
        "    save_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    output_dir=local_output_dir,\n",
        "    optim = \"adamw_8bit\",\n",
        "    warmup_steps = 50,\n",
        "    report_to=\"tensorboard\",\n",
        "    beta=0.1,\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512\n",
        ")\n",
        "trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model=None,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    processing_class=tokenizer\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "FlA9l7Ku8gKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google drive로 복사\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "w8pT5-9QuJd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r {local_output_dir} /content/drive/MyDrive"
      ],
      "metadata": {
        "id": "VgxrHDINuREP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}