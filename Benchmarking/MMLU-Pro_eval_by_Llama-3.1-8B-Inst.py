import re, argparse, os, json
from datasets import load_dataset

def load_json(data_path):
    with open(data_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def is_correct(model_response, gt):
    assert gt != "[invalid]"
    return gt.lower() in model_response.lower()

def is_mcqa_correct(model_response, gt_answer_letter, gt_index, options):
    """
    model_response: Full string a model generates
    There are three different types of answers in the following:
    gt_answer_letter: Alphabetical answers like 'H'
    gt_index: Correct answer index (int)
    options: list of choices
    """
    # Ground truth option string
    gt_text = options[gt_index].strip().lower()

    # Extract answer letter from model response
    predicted_letter = extract_answer_letter(model_response)
    predicted_letter = predicted_letter.upper() if predicted_letter else ""

    # Ground truth answer letter
    gt_letter = gt_answer_letter.upper()

    # Comparison
    is_correct_by_letter = (predicted_letter == gt_letter)
    is_correct_by_text = (gt_text in model_response.lower())

    return is_correct_by_letter or is_correct_by_text

def extract_answer_letter(text):
    """
    Extract the answer letter (A to P) from the model's response.
    Pattern matching priority (in order):
    1. "The answer is (B)"
    2. "Answer: B"
    3. A choice pattern like "A." or "A\n" at the beginning of the response
    4. A single standalone letter (A to P) appearing at the end of the response
    """
    patterns = [
        r"[Tt]he answer is\s*\(?([A-P])\)?",
        r"[Aa]nswer:\s*([A-P])",
        r"[Tt]he best answer is\s*\(?([A-P])\)?",
        r"[Tt]he correct answer is\s*\(?([A-P])\)?",
    ]

    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1)

    # 3. When the response starts with a pattern like "A." or "A\n"
    match = re.match(r"^\s*([A-P])[\.\)]?[\s\n]", text)
    if match:
        return match.group(1)

    return None

def main(args):
    MMLU_Pro = load_dataset("TIGER-Lab/MMLU-Pro")['test']

    # Evaluate
    for output_result in os.listdir(args.output_dir):
        file_path = os.path.join(args.output_dir, output_result)
        with open(file_path, 'r', encoding='utf-8') as f:
            output = json.load(f)

        if 'gsm8k' in output_result:
            correct = 0

        elif 'MMLU-Pro' in output_result:
            correct = 0
            for model_completion, example in zip(output, MMLU_Pro):
                try:
                    model_answer = model_completion['answer']
                    gt_letter = example['answer']           # Example: 'H'
                    gt_index = example['answer_index']      # Ex: 9
                    options = example['options']            # List of choices

                    correct += is_mcqa_correct(model_answer, gt_letter, gt_index, options)
                except Exception as e:
                    print("Error processing example:")
                    print(f"model_answer: {model_completion}")
                    print(f"example: {example}")
                    print(f"Error: {e}")
                    break
            print(f"MMLU=Pro Accuracy for {output_result}: {correct / len(output)}")

if __name__ == "__main__":
    args = argparse.ArgumentParser()
    args.add_argument("--output_dir", type=str, default="outputs")
    args.add_argument("--test_size", type=int, default=100)
    # print(args.output_dir)
    args = args.parse_args()

    main(args)
